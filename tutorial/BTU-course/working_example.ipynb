{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ca3289",
   "metadata": {},
   "source": [
    "# Evaluating CNN Performance on RRAM-based In-Memory Computing Accelerators\n",
    "\n",
    "In this notebook, you will use the [cross-sim](https://github.com/sandialabs/cross-sim) simulator to analyze how the performance of a Convolutional Neural Network (CNN) for MNIST digit recognition is affected when deployed on RRAM-based In-Memory-Computing (IMC) accelerators.\n",
    "\n",
    "You will:\n",
    "- Train and evaluate a CNN in standard PyTorch (software-only baseline).\n",
    "- Evaluate the same trained network using cross-sim to simulate RRAM hardware effects.\n",
    "- Retrain the network using Hardware-Aware Training (HAT) with cross-sim, then evaluate its performance on simulated hardware.\n",
    "\n",
    "Finally, you can draw your own digit and see how each network performs on your input!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691aa8b",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa69bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/TommyR06/cross-sim-BTU-course.git\"\n",
    "REPO_NAME = \"cross-sim-BTU-course/tutorial/BTU-course\"\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone {REPO_URL}\n",
    "%cd {REPO_NAME}\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from applications.mvm_params import set_params\n",
    "import matplotlib.pyplot as plt\n",
    "from simulator.algorithms.dnn.torch.convert import from_torch, reinitialize, synchronize\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(498)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882cdf1-6f03-4f0a-b528-537a6db022d7",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We will use the MNIST dataset. Let's load and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450bb99-fe07-4420-9058-7371a5bbaf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Load the MNIST training set\n",
    "mnist_data = datasets.MNIST(\"./\", download=True, train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              target_transform=transforms.Compose([\n",
    "                                lambda x:torch.tensor([x]), \n",
    "                                lambda x:torch.nn.functional.one_hot(x,10).float(),\n",
    "                                lambda x:x.squeeze(),\n",
    "                                ]))\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_test = datasets.MNIST(\"./\", download=True, train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              target_transform=transforms.Compose([\n",
    "                                lambda x:torch.tensor([x]), \n",
    "                                lambda x:torch.nn.functional.one_hot(x,10).float(),\n",
    "                                lambda x:x.squeeze(),\n",
    "                                ]))\n",
    "\n",
    "# Split dataset into training and validation and create data loaders\n",
    "ds_train, ds_val = torch.utils.data.random_split(mnist_data, [0.8, 0.2])\n",
    "mnist_loader_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "mnist_loader_val = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create test set loader\n",
    "mnist_loader_test = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "N_test = len(mnist_loader_test.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778e087-cca3-4df5-8d90-c3c9b2982471",
   "metadata": {},
   "source": [
    "## 3. Define the CNN Model\n",
    "\n",
    "We will use a simple CNN suitable for MNIST. This is a small network with only 7018 trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73ff1db7-2073-40c3-910a-aa1b9a86a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN topology\n",
    "def mnist_cnn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 8, 3, padding='valid', stride=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(8, 16, 3, padding='valid', stride=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(576, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556fbfd",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions\n",
    "\n",
    "Let's define helper functions for training and evaluating the model.\n",
    "\n",
    "\n",
    "We will use the standard PyTorch wrapper below to train the CNN on MNIST. We will train using the Adam optimizer with a learning rate of $10^{-3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65f424ee-1a81-465c-953b-738e63866c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for training the CNN\n",
    "class SequentialWrapper():\n",
    "    def __init__(self, net, loss, learning_rate=1e-3):\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.forward(batch[0])\n",
    "        loss = self.loss(pred, batch[1])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        pred = self.forward(batch[0])\n",
    "        loss = self.loss(pred, batch[1])\n",
    "        return loss\n",
    "    \n",
    "    def train_epoch(self, train_loader, val_loader):\n",
    "        loss_train, loss_val = 0, 0\n",
    "        for minibatch in iter(train_loader):\n",
    "            loss_train += self.training_step(minibatch).detach()\n",
    "        for minibatch in iter(val_loader):\n",
    "            loss_val += self.validation_step(minibatch).detach()\n",
    "        return loss_train/len(train_loader), loss_val/len(val_loader)\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs):\n",
    "        loss_train, loss_val = np.zeros(epochs), np.zeros(epochs)\n",
    "        for e in tqdm(range(0, epochs)):\n",
    "            lt, lv = self.train_epoch(train_loader, val_loader)\n",
    "            loss_train[e] = lt\n",
    "            loss_val[e] = lv\n",
    "        return loss_train, loss_val\n",
    "\n",
    "# Create the wrapped PyTorch model\n",
    "mnist_cnn_pt = SequentialWrapper(mnist_cnn(), torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685fdc8",
   "metadata": {},
   "source": [
    "## 5. Case I: PyTorch Training and Inference\n",
    "\n",
    "Train and evaluate the CNN using only PyTorch (software baseline).\n",
    "\n",
    "We will first train this CNN as we would normally do in PyTorch, without any analog error injection during training. After training, we'll evaluate the test accuracy, again without any analog errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c31f737-21b6-4388-b651-67f3b68471a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [11:31<00:00, 34.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "No analog errors during training, no analog errors during test\n",
      "Test accuracy: 99.07%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "N_epochs = 20\n",
    "\n",
    "# Train the standard PyTorch CNN\n",
    "loss_train_pt, loss_val_pt = mnist_cnn_pt.train(mnist_loader_train, mnist_loader_val, N_epochs)\n",
    "\n",
    "# Perform inference on the test set, with no analog errors\n",
    "y_pred, y, k = np.zeros(N_test), np.zeros(N_test), 0\n",
    "for inputs, labels in mnist_loader_test:\n",
    "    output = mnist_cnn_pt.net(inputs)\n",
    "    y_pred_k = output.data.detach().numpy()\n",
    "    y_pred = np.append(y_pred,y_pred_k.argmax(axis=-1))\n",
    "    y = np.append(y,labels.detach().numpy().argmax(axis=1))\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_digitalTrain_digitalTest = np.sum(y == y_pred)/len(y)\n",
    "print('===========')\n",
    "print('No analog errors during training, no analog errors during test')\n",
    "print('Test accuracy: {:.2f}%\\n'.format(accuracy_digitalTrain_digitalTest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86516370",
   "metadata": {},
   "source": [
    "## 6. Case II: PyTorch Training, CrossSim Inference\n",
    "\n",
    "Evaluate the PyTorch-trained model using cross-sim to simulate RRAM hardware effects.\n",
    "\n",
    "\n",
    "**MODIFY BELOW BASED ON DEVICE AT IHP**\n",
    "\n",
    "How well does this CNN do when analog errors are injected at inference time? Since this is MNIST, we will simulate inference assuming a memory device that has very large errors. This device will have state-independent conductance errors with $\\alpha = 0.3$. We will disable all other error models to keep this demo simple.\n",
    "\n",
    "We will run inference by first passing our trained CNN through our PyTorch layer converter as we did in Part 2. Since the device error is large, we will simulate inference ten times with re-sampled random device errors each time. This will give us a good statistical picture of the network's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f4f0b-c14f-4187-91e7-7bb82700e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference simulation 10 of 10\n",
      "===========\n",
      "No analog errors during training, CrossSim analog errors during test\n",
      "Test accuracy: 92.00% +/- 2.092%\n"
     ]
    }
   ],
   "source": [
    "# Create a parameters object that models a memory device with very large errors\n",
    "params_analog = set_params(\n",
    "    weight_bits = 8, \n",
    "    wtmodel = \"BALANCED\", \n",
    "    error_model = \"generic\",\n",
    "    proportional_error = \"False\",\n",
    "    alpha_error = 0.3)\n",
    "\n",
    "# Convert the layers in the trained CNN\n",
    "analog_mnist_cnn_pt = from_torch(mnist_cnn_pt.net, params_analog)\n",
    "\n",
    "# Number of inference simulations with re-sampled random analog errors\n",
    "N_runs = 10\n",
    "\n",
    "# Perform analog inference on the test set\n",
    "accuracies = np.zeros(N_runs)\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    y_pred, y, k = np.zeros(N_test), np.zeros(N_test), 0\n",
    "    for inputs, labels in mnist_loader_test:\n",
    "        output = analog_mnist_cnn_pt.forward(inputs)\n",
    "        y_pred_k = output.data.detach().numpy()\n",
    "        y_pred = np.append(y_pred,y_pred_k.argmax(axis=-1))\n",
    "        y = np.append(y,labels.detach().numpy().argmax(axis=1))\n",
    "    accuracies[i] = np.sum(y == y_pred)/len(y)\n",
    "    reinitialize(analog_mnist_cnn_pt)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('No analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_digitalTrain_analogTest = np.mean(accuracies)\n",
    "std_digitalTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(100*accuracy_digitalTrain_analogTest,100*std_digitalTrain_analogTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e03c87",
   "metadata": {},
   "source": [
    "## 7. Case III: CrossSim Training and Inference (Hardware-Aware Training)\n",
    "\n",
    "Retrain the network using cross-sim to include hardware effects during training (HAT), then evaluate on simulated hardware.\n",
    "\n",
    "**MODIFY BELOW BASED ON YOUR CASE**\n",
    "\n",
    "With the inclusion of these large conductance errors, our model loses quite a bit of accuracy on MNIST.\n",
    "\n",
    "Now let's try to see if we can make up this accuracy loss by simulating the conductance errors at inference time during the training process. As before, we will disable all other error models to keep things simple. For a practical hardware-aware training scenario, we can specify our parameters to represent the exact analog hardware configuration that would be used during inference and enable as many different error models in CrossSim as we would like.\n",
    "\n",
    "To do this, we will use a modified training wrapper below that includes only a single new line. The \"synchronize\" method is called after the backward pass to update the conductance values in the AnalogCores with the new updated weight values found using the optimizer. These updated AnalogCores will then be used for the forward pass of the next training epoch.\n",
    "\n",
    "We create another PyTorch CNN, convert its layers to be CrossSim-compatible, then wrap it with the modified training wrapper. Then we will train this model with the same large conductance errors injected during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e5d63e-59ee-4a4c-a42a-3bde325dae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:05<00:00, 60.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Modified training warpper for CrossSim-in-the-loop training\n",
    "class SequentialWrapper_CrossSim(SequentialWrapper):\n",
    "    def __init__(self, net, loss, learning_rate=1e-3):\n",
    "        super().__init__(net, loss, learning_rate)\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.forward(batch[0])\n",
    "        loss = self.loss(pred, batch[1])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        synchronize(self.net)  # <--- The only changed line in all of training!\n",
    "        return loss\n",
    "\n",
    "# Create a PyTorch model with CrossSim-compatible layers\n",
    "analog_mnist_cnn = from_torch(mnist_cnn(), params_analog)\n",
    "\n",
    "# Create the wrapped analog PyTorch model\n",
    "analog_mnist_cnn_CS = SequentialWrapper_CrossSim(analog_mnist_cnn, torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# Train the analog PyTorch model\n",
    "loss_train_CS, loss_val_CS = analog_mnist_cnn_CS.train(mnist_loader_train, mnist_loader_val, N_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a5407-6279-4ab6-9c97-2d16f56356da",
   "metadata": {},
   "source": [
    "Finally, let's perform inference simulation with conductance errors to see if our model that had device-aware training (with the same conductance errors as inference) achieves higher accuracy than the model with standard training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905738f-1b42-4add-8491-7f1f5f7050ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference simulation 1 of 1\n",
      "===========\n",
      "CrossSim analog errors during training, CrossSim analog errors during test\n",
      "Test accuracy: 98.09% +/- 0.000%\n"
     ]
    }
   ],
   "source": [
    "# Perform analog inference on the test set\n",
    "accuracies = np.zeros(N_runs)\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    y_pred, y, k = np.zeros(N_test), np.zeros(N_test), 0\n",
    "    for inputs, labels in mnist_loader_test:\n",
    "        new_input = inputs[0:1]\n",
    "        output = analog_mnist_cnn_CS.net(inputs)\n",
    "        # output = analog_mnist_cnn_CS.net(new_input)\n",
    "        y_pred_k = output.data.detach().numpy()\n",
    "        y_pred = np.append(y_pred,y_pred_k.argmax(axis=-1))\n",
    "        y = np.append(y,labels.detach().numpy().argmax(axis=1))\n",
    "    accuracies[i] = np.sum(y == y_pred)/len(y)\n",
    "    reinitialize(analog_mnist_cnn_CS.net)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('CrossSim analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_analogTrain_analogTest = np.mean(accuracies)\n",
    "std_analogTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(accuracy_analogTrain_analogTest*100,std_analogTrain_analogTest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bca09d-01b9-4415-b642-a69b939d3b88",
   "metadata": {},
   "source": [
    "## 8. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09cbd496-0bfd-4fb1-a9be-64324919fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST test set\n",
      "================\n",
      "Standard training, standard inference: 99.07%\n",
      "Standard training, CrossSim inference: 92.00% +/- 2.092%\n",
      "CrossSim training, CrossSim inference: 98.17% +/- 0.308%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST test set\")\n",
    "print(\"================\")\n",
    "print(\"Standard training, standard inference: {:.2f}%\".format(100*accuracy_digitalTrain_digitalTest))\n",
    "print(\"Standard training, CrossSim inference: {:.2f}% +/- {:.3f}%\".format(100*accuracy_digitalTrain_analogTest, 100*std_digitalTrain_analogTest))\n",
    "print(\"CrossSim training, CrossSim inference: {:.2f}% +/- {:.3f}%\".format(100*accuracy_analogTrain_analogTest, 100*std_analogTrain_analogTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b598d75-614b-4eb0-98ef-2c8fae5a401e",
   "metadata": {},
   "source": [
    "Device-aware training using CrossSim yielded a substantial recovery of the test accuracy in the presence of very large conductance errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a500d5",
   "metadata": {},
   "source": [
    "## 9. Draw Your Own Digit!\n",
    "\n",
    "Use the canvas below to draw a digit (0-9). The image will be preprocessed and fed to all three models. See how each model predicts your digit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec866fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas, hold_canvas\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfc78c",
   "metadata": {},
   "source": [
    "### 9.1. Canvas Generation\n",
    "\n",
    "Re-run the following cell to clear the canvas and re-drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = Canvas(width=512, height=512, sync_image_data=True)\n",
    "canvas.fill_style = 'black'\n",
    "canvas.stroke_style = 'white'\n",
    "canvas.line_width = 20\n",
    "\n",
    "display(canvas)\n",
    "\n",
    "last_pos = [None]\n",
    "def handle_mouse_down(x, y): last_pos[0] = (x, y)\n",
    "def handle_mouse_up(x, y): last_pos[0] = None\n",
    "def handle_mouse_move(x, y):\n",
    "    if last_pos[0] is not None:\n",
    "        with hold_canvas(canvas):\n",
    "            canvas.begin_path()\n",
    "            canvas.move_to(*last_pos[0])\n",
    "            canvas.line_to(x, y)\n",
    "            canvas.stroke()\n",
    "        last_pos[0] = (x, y)\n",
    "\n",
    "\n",
    "canvas.on_mouse_down(handle_mouse_down)\n",
    "canvas.on_mouse_up(handle_mouse_up)\n",
    "canvas.on_mouse_move(handle_mouse_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79424bc",
   "metadata": {},
   "source": [
    "### 9.2. Print the Drawn Symbol\n",
    "\n",
    "Print out the digit from the canvas of the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_canvas_image(canvas, out_size=(28, 28)):\n",
    "    data = np.array(canvas.get_image_data())\n",
    "    gray = np.mean(data[:, :, :3], axis=2).astype(np.uint8)\n",
    "    pil_img = Image.fromarray(gray)\n",
    "    pil_img = pil_img.resize(out_size, Image.LANCZOS)\n",
    "    return np.array(pil_img)\n",
    "\n",
    "output_img = get_canvas_image(canvas)\n",
    "plt.imshow(output_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Output from Canvas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45003a2",
   "metadata": {},
   "source": [
    "### 9.3. Assess Networks Prediction\n",
    "\n",
    "Check the prediction for the drawn symbol of the three networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_tranform = transforms.Compose([\n",
    "    transforms.ToPILImage(),               # Convert to PIL image\n",
    "    transforms.ToTensor()                 # Converts to [C, H, W] and scales to [0.0, 1.0]\n",
    "])\n",
    "\n",
    "img_tensor = writing_tranform(output_img)    # [1, 28, 28]    \n",
    "img_tensor = img_tensor.unsqueeze(0)   # Add batch dimension -> [1, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a8fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Writing Digit\n",
      "================\n",
      "Standard training, standard inference: [3]\n",
      "Standard training, CrossSim inference: [5]\n",
      "CrossSim training, CrossSim inference: [3]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W618 16:53:19.889719448 NNPACK.cpp:57] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "[W618 16:53:19.900309659 NNPACK.cpp:57] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "output_pt = mnist_cnn_pt.forward(img_tensor)\n",
    "output_analog_pt = analog_mnist_cnn_pt.forward(img_tensor)\n",
    "# reinitialize(analog_mnist_cnn_pt) # uncommen to see cycle-by-cycle variability (programming error )\n",
    "output_analog_CS = analog_mnist_cnn_CS.net(img_tensor)\n",
    "# reinitialize(analog_mnist_cnn_CS.net) # uncommen to see cycle-by-cycle variability (programming error )\n",
    "\n",
    "\n",
    "y_pred_output_pt = output_pt.data.detach().numpy()\n",
    "y_pred_output_analog_pt = output_analog_pt.data.detach().numpy()\n",
    "y_pred_output_analog_CS = output_analog_CS.data.detach().numpy()\n",
    "\n",
    "print(\"Predicted Writing Digit\")\n",
    "print(\"================\")\n",
    "print(f\"Standard training, standard inference: {y_pred_output_pt.argmax(axis=-1)}\")\n",
    "print(f\"Standard training, CrossSim inference: {y_pred_output_analog_pt.argmax(axis=-1)}\")\n",
    "print(f\"CrossSim training, CrossSim inference: {y_pred_output_analog_CS.argmax(axis=-1)}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
