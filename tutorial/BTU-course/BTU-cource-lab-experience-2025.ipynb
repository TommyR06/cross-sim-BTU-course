{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa69bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Colab\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/TommyR06/cross-sim-BTU-course.git\"\n",
    "REPO_NAME = \"cross-sim-BTU-course\"\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone {REPO_URL}\n",
    "%cd {REPO_NAME}\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from simulator import AnalogCore\n",
    "from simulator import CrossSimParameters\n",
    "from applications.mvm_params import set_params\n",
    "import scipy.linalg\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from PIL import Image\n",
    "np.random.seed(498)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882cdf1-6f03-4f0a-b528-537a6db022d7",
   "metadata": {},
   "source": [
    "# Part 3: Device-aware training\n",
    "\n",
    "The PyTorch model with CrossSim-compatible layers are not just useful for running inference. They can be trained as well! In this use case, the forward pass through the convolution and fully-connected layers will be executed through CrossSim's AnalogCores, but the backward pass will be idealized, providing a differentiable trace. This means that, to the extent the idealized operation matches the true AnalogCore forward pass, we can perform surrogate gradient descent. This can allow a network to adapt to these non-idealities and recover some of the performance that would be lost from post-training conversion.\n",
    "\n",
    "For this simple demo, we will train on the very simple MNIST dataset, with and without device errors in the loop injected through CrossSim. First, we will load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b78946-b030-4a93-94df-dfd50466b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator.algorithms.dnn.torch.convert import from_torch, reinitialize, synchronize\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4450bb99-fe07-4420-9058-7371a5bbaf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the MNIST training set\n",
    "mnist_data = datasets.MNIST(\"./\", download=True, train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              target_transform=transforms.Compose([\n",
    "                                lambda x:torch.tensor([x]), \n",
    "                                lambda x:torch.nn.functional.one_hot(x,10).float(),\n",
    "                                lambda x:x.squeeze(),\n",
    "                                ]))\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_test = datasets.MNIST(\"./\", download=True, train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              target_transform=transforms.Compose([\n",
    "                                lambda x:torch.tensor([x]), \n",
    "                                lambda x:torch.nn.functional.one_hot(x,10).float(),\n",
    "                                lambda x:x.squeeze(),\n",
    "                                ]))\n",
    "\n",
    "# Split dataset into training and validation and create data loaders\n",
    "ds_train, ds_val = torch.utils.data.random_split(mnist_data, [0.8, 0.2])\n",
    "mnist_loader_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "mnist_loader_val = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create test set loader\n",
    "mnist_loader_test = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "N_test = len(mnist_loader_test.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778e087-cca3-4df5-8d90-c3c9b2982471",
   "metadata": {},
   "source": [
    "We will train a simple three-layer CNN on MNIST, whose topology is defined below. This is a small network with only 7018 trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73ff1db7-2073-40c3-910a-aa1b9a86a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN topology\n",
    "def mnist_cnn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 8, 3, padding='valid', stride=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(8, 16, 3, padding='valid', stride=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(576, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff37ed-c255-4e86-ad37-ff022633c592",
   "metadata": {},
   "source": [
    "We will use the standard PyTorch wrapper below to train the CNN on MNIST. We will train using the Adam optimizer with a learning rate of $10^{-3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f424ee-1a81-465c-953b-738e63866c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for training the CNN\n",
    "class SequentialWrapper():\n",
    "    def __init__(self, net, loss, learning_rate=1e-3):\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.forward(batch[0])\n",
    "        loss = self.loss(pred, batch[1])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        pred = self.forward(batch[0])\n",
    "        loss = self.loss(pred, batch[1])\n",
    "        return loss\n",
    "    \n",
    "    def train_epoch(self, train_loader, val_loader):\n",
    "        loss_train, loss_val = 0, 0\n",
    "        for minibatch in iter(train_loader):\n",
    "            loss_train += self.training_step(minibatch).detach()\n",
    "        for minibatch in iter(val_loader):\n",
    "            loss_val += self.validation_step(minibatch).detach()\n",
    "        return loss_train/len(train_loader), loss_val/len(val_loader)\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs):\n",
    "        loss_train, loss_val = np.zeros(epochs), np.zeros(epochs)\n",
    "        for e in tqdm(range(0, epochs)):\n",
    "            lt, lv = self.train_epoch(train_loader, val_loader)\n",
    "            loss_train[e] = lt\n",
    "            loss_val[e] = lv\n",
    "        return loss_train, loss_val\n",
    "\n",
    "# Create the wrapped PyTorch model\n",
    "mnist_cnn_pt = SequentialWrapper(mnist_cnn(), torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e103747-2c5d-4fd0-9a6b-7c9b1e504992",
   "metadata": {},
   "source": [
    "We will first train this CNN as we would normally do in PyTorch, without any analog error injection during training. After training, we'll evaluate the test accuracy, again without any analog errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c31f737-21b6-4388-b651-67f3b68471a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:46<00:00, 38.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "No analog errors during training, no analog errors during test\n",
      "Test accuracy: 99.16%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "N_epochs = 20\n",
    "\n",
    "# Train the standard PyTorch CNN\n",
    "loss_train_pt, loss_val_pt = mnist_cnn_pt.train(mnist_loader_train, mnist_loader_val, N_epochs)\n",
    "\n",
    "# Perform inference on the test set, with no analog errors\n",
    "y_pred, y, k = np.zeros(N_test), np.zeros(N_test), 0\n",
    "for inputs, labels in mnist_loader_test:\n",
    "    output = mnist_cnn_pt.net(inputs)\n",
    "    y_pred_k = output.data.detach().numpy()\n",
    "    y_pred = np.append(y_pred,y_pred_k.argmax(axis=-1))\n",
    "    y = np.append(y,labels.detach().numpy().argmax(axis=1))\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_digitalTrain_digitalTest = np.sum(y == y_pred)/len(y)\n",
    "print('===========')\n",
    "print('No analog errors during training, no analog errors during test')\n",
    "print('Test accuracy: {:.2f}%\\n'.format(accuracy_digitalTrain_digitalTest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241918cf-568f-474e-bd5d-cb2dd990b6b8",
   "metadata": {},
   "source": [
    "How well does this CNN do when analog errors are injected at inference time? Since this is MNIST, we will simulate inference assuming a memory device that has very large errors. This device will have state-independent conductance errors with $\\alpha = 0.3$. We will disable all other error models to keep this demo simple.\n",
    "\n",
    "We will run inference by first passing our trained CNN through our PyTorch layer converter as we did in Part 2. Since the device error is large, we will simulate inference ten times with re-sampled random device errors each time. This will give us a good statistical picture of the network's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1f4f0b-c14f-4187-91e7-7bb82700e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference simulation 10 of 10\n",
      "===========\n",
      "No analog errors during training, CrossSim analog errors during test\n",
      "Test accuracy: 93.33% +/- 1.606%\n"
     ]
    }
   ],
   "source": [
    "# Create a parameters object that models a memory device with very large errors\n",
    "params_analog = set_params(weight_bits = 8, wtmodel = \"BALANCED\", \n",
    "                         error_model = \"generic\",\n",
    "                         proportional_error = \"False\",\n",
    "                         alpha_error = 0.3)\n",
    "\n",
    "# Convert the layers in the trained CNN\n",
    "analog_mnist_cnn_pt = from_torch(mnist_cnn_pt.net, params_analog)\n",
    "\n",
    "# Number of inference simulations with re-sampled random analog errors\n",
    "N_runs = 10\n",
    "\n",
    "# Perform analog inference on the test set\n",
    "accuracies = np.zeros(N_runs)\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    y_pred, y, k = np.zeros(N_test), np.zeros(N_test), 0\n",
    "    for inputs, labels in mnist_loader_test:\n",
    "        output = analog_mnist_cnn_pt.forward(inputs)\n",
    "        y_pred_k = output.data.detach().numpy()\n",
    "        y_pred = np.append(y_pred,y_pred_k.argmax(axis=-1))\n",
    "        y = np.append(y,labels.detach().numpy().argmax(axis=1))\n",
    "    accuracies[i] = np.sum(y == y_pred)/len(y)\n",
    "    reinitialize(analog_mnist_cnn_pt)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('No analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_digitalTrain_analogTest = np.mean(accuracies)\n",
    "std_digitalTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(100*accuracy_digitalTrain_analogTest,100*std_digitalTrain_analogTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90964fd2-294d-40cb-a59e-9f3eebe94b2b",
   "metadata": {},
   "source": [
    "With the inclusion of these large conductance errors, our model loses quite a bit of accuracy on MNIST.\n",
    "\n",
    "Now let's try to see if we can make up this accuracy loss by simulating the conductance errors at inference time during the training process. As before, we will disable all other error models to keep things simple. For a practical hardware-aware training scenario, we can specify our parameters to represent the exact analog hardware configuration that would be used during inference and enable as many different error models in CrossSim as we would like.\n",
    "\n",
    "To do this, we will use a modified training wrapper below that includes only a single new line. The \"synchronize\" method is called after the backward pass to update the conductance values in the AnalogCores with the new updated weight values found using the optimizer. These updated AnalogCores will then be used for the forward pass of the next training epoch.\n",
    "\n",
    "We create another PyTorch CNN, convert its layers to be CrossSim-compatible, then wrap it with the modified training wrapper. Then we will train this model with the same large conductance errors injected during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e5d63e-59ee-4a4c-a42a-3bde325dae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:20<00:00, 49.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Modified training warpper for CrossSim-in-the-loop training\n",
    "class SequentialWrapper_CrossSim(SequentialWrapper):\n",
    "    def __init__(self, net, loss, learning_rate=1e-3):\n",
    "        super().__init__(net, loss, learning_rate)\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.forward(batch[0])\n",
    "        loss = self.loss(pred, batch[1])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        synchronize(self.net)  # <--- The only changed line in all of training!\n",
    "        return loss\n",
    "\n",
    "# Create a PyTorch model with CrossSim-compatible layers\n",
    "analog_mnist_cnn = from_torch(mnist_cnn(), params_analog)\n",
    "\n",
    "# Create the wrapped analog PyTorch model\n",
    "analog_mnist_cnn_CS = SequentialWrapper_CrossSim(analog_mnist_cnn, torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# Train the analog PyTorch model\n",
    "loss_train_CS, loss_val_CS = analog_mnist_cnn_CS.train(mnist_loader_train, mnist_loader_val, N_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a5407-6279-4ab6-9c97-2d16f56356da",
   "metadata": {},
   "source": [
    "Finally, let's perform inference simulation with conductance errors to see if our model that had device-aware training (with the same conductance errors as inference) achieves higher accuracy than the model with standard training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8905738f-1b42-4add-8491-7f1f5f7050ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference simulation 10 of 10\n",
      "===========\n",
      "CrossSim analog errors during training, CrossSim analog errors during test\n",
      "Test accuracy: 98.27% +/- 0.172%\n"
     ]
    }
   ],
   "source": [
    "# Perform analog inference on the test set\n",
    "accuracies = np.zeros(N_runs)\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    y_pred, y, k = np.zeros(N_test), np.zeros(N_test), 0\n",
    "    for inputs, labels in mnist_loader_test:\n",
    "        output = analog_mnist_cnn_CS.net(inputs)\n",
    "        y_pred_k = output.data.detach().numpy()\n",
    "        y_pred = np.append(y_pred,y_pred_k.argmax(axis=-1))\n",
    "        y = np.append(y,labels.detach().numpy().argmax(axis=1))\n",
    "    accuracies[i] = np.sum(y == y_pred)/len(y)\n",
    "    reinitialize(analog_mnist_cnn_CS.net)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('CrossSim analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_analogTrain_analogTest = np.mean(accuracies)\n",
    "std_analogTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(accuracy_analogTrain_analogTest*100,std_analogTrain_analogTest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bca09d-01b9-4415-b642-a69b939d3b88",
   "metadata": {},
   "source": [
    "Let's summarize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09cbd496-0bfd-4fb1-a9be-64324919fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST test set\n",
      "================\n",
      "Standard training, standard inference: 99.16%\n",
      "Standard training, CrossSim inference: 93.33% +/- 1.606%\n",
      "CrossSim training, CrossSim inference: 98.27% +/- 0.172%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on MNIST test set\")\n",
    "print(\"================\")\n",
    "print(\"Standard training, standard inference: {:.2f}%\".format(100*accuracy_digitalTrain_digitalTest))\n",
    "print(\"Standard training, CrossSim inference: {:.2f}% +/- {:.3f}%\".format(100*accuracy_digitalTrain_analogTest, 100*std_digitalTrain_analogTest))\n",
    "print(\"CrossSim training, CrossSim inference: {:.2f}% +/- {:.3f}%\".format(100*accuracy_analogTrain_analogTest, 100*std_analogTrain_analogTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b598d75-614b-4eb0-98ef-2c8fae5a401e",
   "metadata": {},
   "source": [
    "Device-aware training using CrossSim yielded a substantial recovery of the test accuracy in the presence of very large conductance errors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
